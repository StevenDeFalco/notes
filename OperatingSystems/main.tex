\documentclass{article}


% Packages...
\usepackage{listings} % to make nicely formatted code blocks
\usepackage{hyperref} % to get hot links in the table of contents
\usepackage{xcolor}   % to define custom colors
\usepackage{titlesec} % to make custom section formatting 


% Custom Commands...

% \bold{} to bold the following text
% usage: \bold ==> \bold{bolded text}
\newcommand{\bold}[1]{\textbf{#1}}

% \b to create a new list item
% usage: \b this is a bullet in my list
\renewcommand{\b}{\item[$\circ$]}

% \newlist to start an itemized list
% usage: \newlist \\ \b bullet 1 ...
\newcommand{\newlist}{\begin{itemize}}

% \listend to end an itemized list
% usage: ... \b {last bullet} \\ \endlist
\renewcommand{\endlist}{\end{itemize}}

% \code to format inline strings of code
% usage: \code{print("Hello world!")}
\newcommand{\code}[1]{\texttt{#1}}


% formatting defaults...

% removes paragraph indent by default
\setlength{\parindent}{0pt}

% custom colors for the code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{light-gray}{gray}{0.95}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% sets environment defaults for lstlistings (code blocks)
\lstset{
    backgroundcolor=\color{light-gray},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    numbers=left,       
    numbersep=10pt,                  
    tabsize=2,
    frame=tb,
    stepnumber=1,
}

% sets custom paragraph environment
% layer of division that can be used with subsubsections
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\title{CS492: Operating Systems \\ Notes}
\author{Steven DeFalco}
\date{Spring 2023}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Introduction}

\subsection{Operating System Concepts and Structure}

    The \textbf{operating system} offers functionality through \textbf{system calls}. When a system call is made, the kernel runs appropriate code in the \textbf{priveleged mode}. \\

    \noindent A group of system calls implements \textbf{services} such as file system serives and process management services. \\

    \subsubsection{Process vs. Program}

    \noindent A \textbf{process} is a user-level abstraction to execute a program on behalf of a user. Process is a kernel abstraction in which the program is going to run. Each process has its own \textbf{address space}. A \textbf{program} is a series of binary CPU instructions stored in a file. A program requires resources to be executed (such as CPU or memory). The operating system needs to represent an instance of a program in execution and that is a process. 

    \subsubsection{Address Space}

    \noindent \textbf{Address space} is the range of valid memory address for a given process. Address space can be read and/or written and/or executed. Each process has its own separate address space which consists of...
    
    \begin{center}
        \bold{Text} - binary program code \\

        \bold{Stack} - function calls data \\

        \bold{Heap} - dunamically allocated data \\
    \end{center}

    \subsubsection{Files}

    \noindent A \textbf{file} is an abstraction of a (possible) real storage device such as a hard disk. You can read/write data from/to a file by providing postion and an amount of data to transfer. In UNIX, everything is considered a file. Files are maintained in \textbf{directories} which keep a name for each file they contain. Directories and files form a hierarchy. 

    \newlist

\item \textbf{Block Special Files} can be written and read from block by block. (e.g. disk)

\item \textbf{Character Special Files} can be written and read from byte by byte. (e.g. serial port)

\item \textbf{Pipes} are pseudo files allowing for multiple process to communicate over a FIFO channel. 

    \endlist

    \noindent \textbf{Device drivers} are software inside the kernel that knows how to initialize and communicate with specific hardware devices. 

    \subsubsection{System Calls}

    \textbf{System calls} are the interface the kernel offers to applications to issue service requests. They are highly specific to the operating system and hardware; therefore, you should use system calls included in the C standard library. If you use C system calls then your code will be portable because C will use the correct internal system call depending on the operating system without requiring the programmer to use the operating system specific call. 

    \subsubsection{Kernels}

    \textbf{Monolithic kernel} is a very large collection of C functions linked together into a single very large executable binary program that executes in kernel mode.

    \begin{itemize}
    \item every function can call every other function
    \item a single bug will crash everything
    \item historically how most kernels were implemented
    \end{itemize}

    \noindent \textbf{Microkernel} splits the kernel into small well-defined modules. 


    \begin{itemize}
    \item Only one kernel runs in kernel mode: the microkernel
    \item the rest of the modules run as relatively powerless ordinary user processes
    \item a single bug cannot crash the entire system
    \item there is more overhead communication between the modules
    \end{itemize}

    \noindent \textbf{Hybrid kernel} is a compromise between a microkernel and a monolithic kernel

\subsection{Computer Hardware}

    \textbf{Central Processing Unit (CPU)}

    \begin{itemize}
    \item fetches instructions from memory and executes them
    \item each CPU has a specific set of instructions and thus different instruction sets on different CPUs will not be interchangeable
    \item has different \textbf{registers}
    \begin{itemize}
        \item \bold{General} register for genera data
        \item \bold{Program} counter to store the memory address of the next CPU instruction to execute
        \item \bold{Stack pointer} to store the address of the top of the stack
        \item \bold{Program status word}: readable/writable bits that store the state of the CPU and different conditions that resulted from the previous instruction
    \end{itemize}
    \end{itemize}

    \noindent \textbf{Memory}

    \noindent In order of decreasing speed and increasing capacity: registers, cahce, main memory, magnetic disk, tape

    \begin{itemize}
    \item the cache temporarily holds a piece of data from a slower memory into a faster memory
    \end{itemize}

    \noindent \textbf{Hardware multithreading} is when you run multiple processes on a single core. Each core has two (or more) sets of registers. Switches between two different sets of registers on each instruction (where needed) to eliminate waiting for data. \\

    \noindent \textbf{Flash memory} stores data even when the electricity is removed (SSD). \\

    \noindent An \textbf{interrupt} is an electric signal on the bus. Interrupt processing involves taking the interrupt, running the kernel interrupt handler, and returning to the user process. The CPU checks for interrupts in-between instructions. When there is an interrupt, the CPU will jump to a unique function in the kernel (the interrupt handler), the kernel handles the interrupt, then the CPU resumes the original process. 

\subsection{Introduction to Linux}

    The C standard library contains the implementation of all the functions required by the definition of the C programming language. The kernel provides an API which is also made of C functions, called system calls. Calling a system call cannot be done directly in a user program written in C because it requires use of a special assembly instruction that switches the CPU to kernel mode. Libc includes C functions (wrappers) which have the same names as the system calls which internally do the real system call for you. If such a wrapper is not available for a given system call, libc also provides the \emph{syscall} function that allows you to call a system call using its system call number. \\

    \noindent \textbf{POSIX (Portable Operating System Interface)} defines a portable interface including multiple aspects of the OS interface to maintain compatibility between different OSes. \\

    \subsubsection{Create your own kernel}

    \begin{enumerate}
    \item Configure the kernel by hand (\texttt{make menuconfig})
    \item Compile the kernel (\texttt{make -h \$(nproc) all})
    \item Install the modules (\texttt{sudo make INSTALL\_MOD\_STRIP=1 modules\_install})
    \item Install the kernel (\texttt{sudo make install})
    \end{enumerate}
    
    \noindent \textbf{Modules} are a piece of compiled C code added to the kernel while the kernel is already running, in kernel mode. Modules shorten the development cycle because modules can be loaded and unloaded without having to reboot the system. To load a module that you've made use \texttt{sudo insmod ./<module-name>.ko} and to remove the module use \texttt{sudo rmmod <module-name>}. Modules have the visibility of all the kernel, so you can use all global variables and exported functions. 

\subsection{System Calls}

    \textbf{System calls} are the only way a process can enter the kernel. Used to request OS services and priveleged operations such as accesing the hardware, creating other processes, and changing security permissions. System calls switch the execution context of the GPU from user mode to kernel mode. \\

    \noindent \textbf{Syscall trap} / Interrupt / Exception

    \begin{itemize}
    \item hardware stacks program counter
    \item hardware loads new program counter from interrupt vector
    \item assembly language procedure saves registers
    \item assembly language procedure sets up new stack
    \item C interrupt service runs
    \item scheduler decides which process is to run next
    \item C procedure returns to the assembly code
    \item assembly language procedure starts up new current process
    \end{itemize}

    \subsubsection{To add a syscall in Linux}

    \begin{enumerate}
    \item Write your syscall function in either an existing file or a new file. Modify the makefile (\texttt{obj-y += my\_syscall.o})
    \item Add your syscall in the architecture specific syscall table (
    \texttt{arch/x86/entry/syscall/syscall\_64.tbl})
    \item Add your ssycall prototype (\texttt{include/linux/syscalls.h})
    \item Recompile, reinstall, reboot the kernel
    \item Test using the syscall function: invoking the system call from a user-mode program to trigger your code
    \end{enumerate}

\newpage
\section{Processes and Threads}

\subsection{Introduction to Processes}

    Multiple processes can run the same program. A single CPU can run only one process at a time. \\

    \noindent \textbf{Multiple processes (multiprogramming)} increases CPU utilization. This consists of overlapping one process's computation with another's wait and also reduces latency (more interactive). To distinguish multiple processes of the same program, each process has a different \textbf{process identifier (PID)}. \\

    \noindent A \textbf{context switch} pauses execution of one process and continues with the execution of another process. The order in which processes execute is not fixed and may not be reproducible. Thus, programmers cannot make time assumptions and must write their code as if it is the only thing that will be running on the CPU. \\

    \noindent Each process has the following key components: 

    \begin{itemize}
    \item \textbf{stack} stores function arguments and local variables
    \item \textbf{heap} stores dynamically allocated memory
    \item \textbf{executable program} is made up of the initialized data segment and the text segment (CPU instructions from program file)
    \item \textbf{execution context} contains program counter, stack pointer, CPU registers, etc...
    \end{itemize}

    \noindent \textbf{Memory layout randomization} mixes the location of each part in memory so that the OS is more secure. It will be harder to locate specific processes in memory if it is randomized. There will also be unusable memory in the process that is there to kill the process if anything tries to access the unusable memory. \\

    \noindent \textbf{Stack} is built with stack frames and includes local (function) variables). Function parameters are passed on the stack. (LIFO) \\

    \noindent \textbf{Heap} is used if it is not known how much data you will need at runtime. Dynamically allocated with random access. \\ 

    \noindent \textbf{Processes} are created during system initialization, from already running processes, due to user interaction, or execution of a scheduled batch job. UNIX process philosophy is to create a new process which is an identical copy of the current process (fork) and then in the new process load the other program. \\

    \noindent \texttt{fork()} is a syscall used to create a new process

    \begin{itemize}
    \item the proces that calls the \texttt{fork()} syscall is called the \textbf{parent process}
    \item the newly created process is called the \textbf{child process}
    \item the child is an identical copy of the parent (runs the exact same code)
    \begin{itemize}
    \item child process returns a zero value
    \item parent process reutrns the PID of the new child
    \end{itemize}
    \item If C code contains a sequence of $n$ consecutive \texttt{fork()} calls, $2^n$ processes will be created
    \item most UNIX systems have a limit on the number of processes a given user can have at the same time (\texttt{ulimit -a})
    \end{itemize}
    
    \noindent A processes \textbf{execution state} is what the process is currently doing...

    \begin{itemize}
    \item \textbf{Running} - executing instructions on the CPU, it is the process that currently has control of the CPU, cannot have more processes running than number of cores/CPUs
    \item \textbf{Ready} - waitng to be assigned to CPU: ready to execute, but another process is executing on the CPU
    \item \textbf{Waiting/blocked} - waiting for an event: cannot make progress until event happens (most processes are in this state)
    \end{itemize}

    \noindent Types of \textbf{process termination}
    \begin{itemize}
    \item Normal exit (voluntary)
    \item Error exit (voluntary)
    \item Fatal error (involuntary) is used when there is a fatal program bug
    \end{itemize}

    \noindent \textbf{Orphan} processes are children processes whose parents have been terminated. Can be adopted by another process, or can be killed automatically when parent is killed (for security reasons). \\

    \noindent Kernel maintains a table called the \textbf{process table}. Is stored in the kernel memory and is invisble to processes. Has one entry per process (process's PID is an index into this table). The entry is called the \textbf{process control block (PCB)}: contains all information about a process. \\

    \noindent When a \textbf{context switch} occurs
    
    \newlist
    \b hardware stacks program counter
    \b hardware loads new program counter from interrupt vector
    \b assembly language procedure saves registers 
    \b assembly language procedure sets up new stack
    \b C interrupt service runs
    \b scheduler decides which process is to run next
    \b C procdure returns to the assembly code
    \b assembly language procedure starts up new current process
    \endlist

\subsection{Processes in Linux}

    Every process has a user and a kernel part in its virtual address space. User space is running the program; kernel space is running kernel code on behalf of the program (kernel space is invisble to the program). There will be both a user-space and kernel-space \textbf{execution context} and thus there will be 2 PCs, 2 stack pointer, 2 stacks, etc... \\

    \noindent In Linux, a process is a \textbf{task} represented by a \texttt{task\_struct} inside the kernel's memory. This is the \textbf{proces control block} for a linux task. A task struct includes:

    \newlist
    \b scheduling parameters
    \b memory image
    \b signals
    \b CPU registers
    \b system call state
    \b file descriptor table
    \b accounting
    \b kernel stack
    \endlist

    \noindent \texttt{fork()} is limited to creating processes only. On Linux, task creation in general can be done by the \texttt{clone()} syscall. When \texttt{clone()} is called... 

    \newlist
    \b new child starts executing function
    \b may use a new stack
    \b has \texttt{sharing\_flags} to describe the amount of sharing between the caller and the callee
    \b blurs the disctinction between process and thread creation 
    \newlist
    \b if you share nothing then it's the same as \texttt{fork()}
    \b if you share everything then it's the same as thread creation
    \endlist
    \endlist

    \noindent Internally, both \texttt{fork()} and \texttt{clone()} call the kernel function \texttt{kernel\_clone()}.

\subsection{Threads}

    A \textbf{thread} cannot exist without a process. A process is a "container" for threads. A process may contain one (the default) or more threads. Thread tells you what part of the program is currently being executed. \textbf{Thread} is an independent sequential execution stream within a process; programs use one or more threads per process. Items that are \textbf{shared per process} include the following:

    \newlist
    \b address space
    \b global variables
    \b open files
    \b child processes
    \b pending alarms
    \b signals and signal handlers
    \b acccounting information
    \endlist

    \noindent Items that are \textbf{shared per thread} (private to each thread) include the following:

    \newlist
    \b program counter
    \b registers 
    \b stack
    \b state
    \endlist
    
    \noindent Some of the advantages of threads:  \\
    \textbf{Performance} - shared address space and therefore no communication overhead between threads. Thread creation can be 10-100 times faster than process creation. \\
    \textbf{Efficiency} - allows a program to overlap I/O and communication. Allows one process to use multiple cores. \\

    \noindent \textbf{Amdahl's Law} identifies performance gains from adding additional cores to an application that has both serial and parallel components. \textbf{Serial} components can only be executed by a single thread. \textbf{Parallel} components can be executed by multiple threads. 
    $$speedup \leq \frac{1}{S + \frac{(1-S)}{N}}$$
    where S is the serial portion and N is the number of processing cores. Note that the serial portion of an application has disproportionate effect on performance gained by adding additional cores / CPUs. \\

    All threads in a single process share the same address space. Global variables are shared between threads. Multiple threads may access the same global variable concurrently and programers are responsible to coordinate access (synchronization algorithms). Each thread gets its own local variables (in stack). Each process starts with a single "main" thread; the first thread can create new threads. Any thread can end at any point; if the main thread returns from the \texttt{main()} function, the \texttt{exit()} system call is called and the whole process terminates. \\
    
    \subsubsection{PCB}

    When making threads, the kernel will break the PCB into two pieces.

    \begin{enumerate}
    \b Information about program execution is stored in \textbf{Thread Control Block (TCB)}. This includes the program counter, CPU registers, scheduling information, and pending I/O information. 
    \b Other information is stored in the \textbf{Process Control Block (PCB)}. This includes memory management information and accounting information. 
    \end{enumerate}

    \subsubsection{Threads Implemented in User Space}

    \textbf{Pros}:
    \newlist
    \b \textbf{Portable}: can be used on any OS
    \b a user-level threads library can be implemented on an OS that does not support threads
    \b thread switching is at least an order of magnitude faster than trapping to the kernel
    \b thread scheduling is very fast: no process context switching, no kernel trap, no flushing of memory cache
    \b each process can have its own thread scheduling algorithm
    \endlist

    \noindent \textbf{Cons}:
    \newlist
    \b \textbf{Doesn't support real parallelism}: kernel will schedule process on single core regardless of number of threads and therefore cannot take advantage of multicore/multiprocessor systems
    \b if one thread makes a blocking system call then the kernel suspends the whole process
    \b threads need to voluntarily give up the CPU to each other for multiprogramming so user code is more complex
    \endlist

    \subsubsection{Threads Implemented in Kernel Space}

    \textbf{Pros}:
    \newlist
    \b \textbf{supports real parallelism} (multicore/multiprocessor systems)
    \b no run-time thread system needed in each process
    \b no thread table in each process
    \b block system calls are not a problem (the kernel can schedule another thread)
    \endlist

    \noindent \textbf{Cons}:
    \newlist
    \b \textbf{slower than user-space threads}: if thread operations are common, much more kernel overhead will be incurred because of the many system calls required
    \b if you fork a multithreaded process, do you copy all threads or just the on that called \texttt{fork()}?
    \b if signals are sent to processes, should the kernel assign it to a specific thread to handle
    \endlist

    \noindent In practice, user threads are mapped one-to-one with kernel threads: \textbf{hybrid implementation}. 

    \subsubsection{Context Switching with Threads}

    When context switching with threads...

    \newlist
    \b thread is now the unit of a context switch
    \b context switch causes CPU state to be copied to/from the TCB
    \b when context switching two threads of the same process, no need to change address space
    \b when context switching two threads in different processes, must change address space 
    \endlist

    \subsubsection{Thread Local Storage}

    \textbf{Thread local storage (TLS)} works like a global variable but each thread has its own copy of it. Use this instead of global variables when making single-threaded code multi-threaded. Visible everywhere in the code, but each thread has its own copy. 

\subsection{Threads in Linux}

    Linux, implements threads in the kernel, not in user space. Linux uniformally handles processes and threads: processes and thread tables are unified into a single data structure. Processes and threads are considered \textbf{tasks} and are thus represented by a \texttt{task\_struct}: there is one \texttt{task\_struct} per thread. Linux assigns PID and TID for each thread; each thread has a unique TID. To check the TID use the \texttt{gettid()} syscall. 

\section{Scheduling}

\subsection{Introduction to Scheduling}

    Multiple processes and threads are ready to run; the kernel's scheduler decides which runs next. Scheduling decisions may take place when a process/thread is created, terminates, blocks on an event. Scheduling decisions may also take place when an interrupt occurs; this includes clock interrupts ($running$ to $ready$) or I/O interrupts ($blocked$ to $ready$). 

    \subsubsection{Clock Interrupts}

    \textbf{Clock interrupts} are a way for a system to keep track of time. Interrupts occur at periodic intervals called a clock tick. Clock interrupts are implemented using a hardware clock interrupt and have high priority in the system. 

    \subsubsection{Non-preemptive Scheduling}
    
    In \textbf{Non-preemptive Scheduling}, proceses execute until completion or until they make a \textbf{voluntary process swtich} or \textbf{process switch on blocking calls}. The scheduler gets involved only at exit or on request (i.e. for every clock interrupt, running processes keep going).

    \subsubsection{Preemptive Scheduling}

    In \textbf{preemtive scheduling} while a process executes, its execution may be paused and another process resumes its execution. This can take the form of an \textbf{involuntary process switch}. This scheduling is used in all OSes today. 

    \subsubsection{Goals of All Scheduling Algorithms}

    \newlist
    \b \textbf{fairness} - give each process a fair share of the CPU
    \b \textbf{policy enforcement} - seeing that stated policy is carried out
    \b \textbf{balance} - keeping all parts of the system busy
    \endlist
        
\subsection{Scheduling in Batch Systems}

    \subsubsection{Goals for Scheduling in Batch Systems}
    \newlist
    \b \textbf{throughput} - maximize jobs per hour
    \b \textbf{turnaround time} - maximize time between submission and termination
    \b \textbf{CPU utilization} - keep the CPU busy all the time
    \endlist

    \subsubsection{First-come First-served (FCFS)}

    Processes are assigned to the CPU in the order they request it (or they arrive). The non-preemptive version will let each job keep running until completion. \textbf{Convoy effect} is when a long process delays short processes. \textbf{Turnaround time} is the time taken by a job to complete after submission (including the wait time). 

    $$\textrm{turnaround time} = \textrm{time}_{\textrm{end}} - \textrm{time}_{\textrm{submitted}}$$
    
    \subsubsection{Shortest Job First (SJF)}

    Associate the length of its CPU time with each process. Use the CPU time length to schedule the process with the shortest CPU time first. In the \textbf{non-preemptive version}, once the CPU is given to the process, it cannot be taken away and then after completion of a process, will consider the next shortest job and begin executing that one until completion. In the \textbf{preemptive version}, if a new process arrives with less CPU time than the remaining time of the current ecxecuting process: then preempt. \\

    \noindent The \textbf{preemptive version} is called \textbf{shortest remaining time next (SRTN)} and is optimal in terms of average turnaround time (i.e. always gives minimum average turnaround time). This method also prevents the convoy effect. However, there are more context switches and a process may be starved of the processeor if short processes keep arriving. 

\subsection{Scheduling in Interactive Systems}
    
    \subsubsection{Goals for Scheduling in Interactive Systems}

    \newlist
    \b \textbf{response time} - respond to requests quickly
    \b \textbf{proportionality} - match duration expectation of users
    \endlist

    \subsubsection{Round-Robin (RR)}

    Each process is allowed to run for a specified time interval called the \bold{time quantum}; in practice, the time quantim is likely not fixed. After this time (the time quantum) has elapsed, the process is preempted and added to the end of the ready queue and the next process is scheduled. If the process terminates or blocks before the time quantum is entirely used up, then the process loses the rest of its time quantum and the next process is scheduled with a new time quantum. \\
    
    \noindent \bold{Advantages of RR}

    \newlist
    \b solution to fairness and starvation
    \b fair allocation of CPU across jobs
    \b low average waiting time when job lengths vary
    \b good for responsiveness if small number of jobs
    \endlist

    \noindent \bold{Disadvantages of RR}

    \newlist
    \b context-switching time may add up for long jobs
    \endlist 

    \noindent \bold{Context switching} may impact the choice of the time quantum...

    \newlist
    \b when there are many processes, a long quantum causes a poor response time
    \b a short time quantum makes things more responsive but gives a higher context switch overhead
    \b time quantum might be variable depending on CPU load
    \endlist

    \noindent \bold{Cache state} must be shared between all jobs which may slow down execution in RR scheduling. There is no cache sharing in FCFS. 

    \subsubsection{Priority (PRIO) Scheduling}

    \bold{PRIO} has four priority classes and always executes the highest-priority runnable jobs to completion. Each queue is processed in a round-robin fashion with a time quantum. Note that lower numbers represent higher priority. There are, however, some \bold{problems} with PRIO scheduling...

    \newlist
    \b \bold{starvation} when lower priority jobs don't get run because higher priority tasks always running
    \b \bold{priority inversion} happpens when a low priority task has resource needed by high priority task, which then must wait
    \endlist

    \noindent Priorities can be \bold{assigned} \bold{statically} based on process type, user, how much the user paid (if a cloud server); or \bold{dynamically} based on how much a process runs vs doing I/O. $\textrm{Priority} = \frac{1}{f}$ where $f$ is the size of the quantum last used. Thus, the longer a process ran, the lower its priority and the process that runs the shortest gets the highest priority to run next.

    \subsubsection{Multiple Queues (MQ) Scheduling}

    Same as priority scheduling, but each queue has a different time quantum (shortest quantum for high-priority and longer for low-priority). Processes start at the highest priority. When a process \bold{exceeds} its time qunatum, it's moved to the next lowest priority queue. When a process \bold{becomes interactive} it is moved to the higher priority. If the user discovers how to make their tasks more ineractive, they can get all of their processes to be very high priority, which is a problem. 

    \subsubsection{Other Schedulers for Interactive Systems}

    \newlist
    \b \bold{Shortest processes next}: SJF but educated guess for how much CPU a process will need next
    \b \bold{Guaranteed scheduling}
    \b \bold{Lottery scheduling} (probabilistic scheduling)
    \b \bold{Fair-share scheduling}: round robin scheduling between users. 
    \endlist

\subsection{Scheduling in Real-time Systems}
    
    \subsubsection{Goals for Scheduling in Real-Time Systems}

    \newlist
    \b \textbf{meeting deadlines} - avoid losing data
    \b \textbf{predictability} - avoid quality degradation in multimedia systems
    \endlist

    \noindent Time plays an essential role in real-time system scheduling. One or more physical devies external to the computer generate events and the computer must react appropriately to them within a fixed amount of time. If the computer reacts too late, it is \bold{as bad as not reacting}. \\

    \noindent Real-time scheduling can be split into two categories. \bold{Hard real-time} is when there are absolute deadlines that must be met. \bold{Soft real-time} is when missing an occassional deadline is undesirable but tolerable. \\ 

    \noindent Let's assume that process behavior is predictable and known in advance (the software should be writtein in a way to make this true). Task timing is known and represented through the following values. 
    
    \begin{center}
        \bold{Release time} ($R_i$) is the earliest time when a task can start execution \\
        \bold{Execution time} ($C_i$) is the expected execution time for a task \\ 
        \bold{Deadline} ($D_i$) is the time by which the proecss must be completed. 
    \end{center}

    \noindent We can also assume that the \bold{periodicity} is known. It can be one of the following options:

    \newlist
    \b periodic process
    \b sporadic process (aperiodic, hard deadlines)
    \b aperiodic proces (aperiodic, soft deadline)
    \endlist

    \subsubsection{Number of Schedulable Processes}

    $$\sum_{i=1}^{m} \frac{C_i}{P_i} \leq 1$$

    \noindent where ther are $m$ periodic events. Event $i$ occurs with period $P_i$, requires $C_i$ time on the CPU. The percentage of CPU usage for event $i$ is $\frac{C_i}{P_i}$. 

    \subsubsection{Rate Monotonic Scheduling}

    Preemptive algorithm, where the shorter the period, the higher the priority.

    \subsubsection{Earliest Deadline First Scheduling}

    Schedules processes according to the shortest remaining time until the next deadline. The shorter the remaining time, the higher the priority. 

\section{Concurrency}

\subsection{Inter-process Communication}

    Processes need to share information or coordinate. Threads are nice for this but do not provide isolation from each other. Processes provide isolation but communication requires system calls. Some \bold{problems with inter-process communication} include...

    \newlist
    \b how can one process pass information to another in such a way that porcesses do not get in each other's way?
    \b how to maintain proper sequencing when dependencies are present between processes?
    \endlist 

    \noindent The same problems apply to inter-thread communication because by design, threads communicate via shared memory. For one process to \bold{pass information} to another, it can...

    \newlist
    \b pass messages through the kernel
    \b share memory (which can be setup with system calls)
    \b share a file 
    \b use asynchronous signals or alerts
    \b use named pipes (which is just a message queue inside the kernel essentially)
    \endlist 
    
\subsection{Race Conditions}

    A \bold{race condition} occurs when two or more processes/threads are reading or writing shared data and the final result depends on which runs precisely when. The \bold{critical section} is the part of the program where the shared data is accessed. Uncoordinated read/write of the data in critical sections may lead to race conditions. Must find some way to prohibit more than one process to execute in its critical section at the same time. A solution is to have \bold{mutual exclusive} access to critical sections: only one processor or thread in a critical section at any time. 

    \subsubsection{Requirements to Avoid Race Conditions}

    \begin{enumerate}
    \b No two processes may be simultaneously inside their critical section: \bold{mutual exclusion}
    \b No processes running outside its critical section may block other processes from entering a critical section: \bold{progress}
    \b No process should have to wait forever to enter its critical section (be starved): \bold{bounded waiting}
    \end{enumerate}

    \subsubsection{Disabling Interrupts to Avoid Race Conditions}
    
    Right before entering the critical section, the process disables all hardware interrupts and when leaving the critical section, the process reenables all interrupts. When interrupts are disabled, the CPU cannot be switched to another process, so the current process can keep the CPU just for itself while it is in the critical section (unless the process makes a system call). The \bold{advantage} of disabling interrupts is that it is very easy to implement in softare. The \bold{disadvantages} of disabling interrupts is that it is unwise to give user processes the ability to turn off interrupts and this is not suitable for multicore systems. In practice, disabling interrupts is only used on single core machines for mutual exclusion in kernel space. 
    
    \subsubsection{Using Lock Variables to Avoid Race Conditions}

    Use a single shared variable (\bold{lock}) between processes. A value of 0 means that no process is in its critical section. A vaue of 1 means that some process is in its critical section. This solution creates a critical section with the lock if the lock is read and written to in two separate lines of code (i.e. if the process context switches after reading the lock but before writing/updating the lock, then both processes may enter critica sectin simultaneously). Continuously testing a variable until some value appears (\bold{busy waiting}) wastes a lot of CPU time. Note that a lock variable that uses busy waiting is called a \bold{spinlock}. 

    \subsubsection{Strict Alternation to Avoid Race Conditinos}

    There is a single turn variable shared between processes. A value of 0 means that it is the turn of process 0. A value of 1 means it is the turn of process 1. the turn variable is initially set with 0 or 1 based on how should start first and the processes strictly alternate. This can easily be generalized to $n$ different processes. An \bold{advantage} of this solution is that there are no race conditions. A \bold{disadvantage} of this solution is that it does not achieve progres because a process can be blocked by a process not in its critical section.

    \subsubsection{Peterson's Solution}

    Use two shared variables \texttt{(turn, flags[])} between processes. \texttt{int turn} imposes an access order: which process can enter its critical section next. \texttt{bool flags[]} (array) specifies for each process whether it is currently interested in entering its critical section. This array is initialized to be false. Some \bold{advantages} of this solution are that there are no race conditions and it satisfies all three conditions for critical regions. \bold{Disadvantages} are that busy waiting uses a lot of CPU and generalization to more than two processes is complex. 

    \subsubsection{Test-and-Set Lock (TSL) Instruction}

    Reads and modifies the content of a memory word atomically. Takes arguments \texttt{TSL register, memory\_address}. Returns in a register the current value of the memory word at \texttt{memory\_address} and then sets the value of the memory word at \texttt{memory\_address} to true (usually abstracted as a non-zero value). Operations cannot be interrupted during execution because this locks the memory bus. Essentially, there can only be one read and one write in one atomic step. 

    \subsubsection{TSL Solution}

    There is a single lock variable in memory at \texttt{LOCK\_ADDR}, shared between processes. Uses TSL instead of while loops and assignment. \bold{Advantages} of this solution are that there are no race conditions, mutual exclusion is achieved, and progress can be made. \bold{Disadvantages} of this solution are that you must have a TSL instruction implemented in the CPU, there is no bounded waiting, and it has busy-waiting. 
        
    \subsubsection{Producer-Consumer Problem with \texttt{sleep()} and \texttt{wakeup()}}

    Instead of busy-waiting, let the process sleep (requires kernel syscalls). The \texttt{sleep()} syscall causes the caller to give up the CPU for some duration of time or until some other process wakes it up. The \texttt{wakeup()} syscall causes the caller to wake up some sleeping process. Note that this solution doesn't sovle the priority inversion problem because process B might still end up having to wait a long time for process A to exit the critical section, even if process B is now sleeping

    \begin{lstlisting}[language=C]
    Producer
    while(1){
        produce an item A;
        if (count == N) //full buffer
            sleep();
        insert item; 
        count++;
        if (count == 1) //was buffer empty
            wakeup(consumer);
    }

    Consumer
    while(1){
        if (count == 0) //empty buffer
            sleep();
        remove item;
        count--;
        if (count == N-1) //was buffer full?
            wakeup(producer);
        consume an item;
    }
    \end{lstlisting}

    \noindent In this soution both the producer and consumer may sleep forever. An \bold{advantage} of this solution is that there is no busy-waiting. \bold{Disadvantages} of this solution are that there is a race condition. A wakeup sent to a process that is not (yet) sleeping is lost. Decision to go to sleep and calling sleep() must be paired. So that the test of \texttt{count} must be done at the same time as the sleep/wakeup in some way. To achieve this we use \bold{semaphores}...

    \subsubsection{Sempahores}

    The purpose of \bold{sempahores} is to solve the lost wakeup problem (explained just above) and avoid race conditions. Semaphores will try to store the number of wakeups. To make a semaphore, define a count variable (the semaphore): \texttt{Down(<semaphore>)} is equivalent to consume (decrease) or sleep(), \texttt{Up(<semaphore>)} is equivalent to produce (increase) and (potentially) wakeup. \texttt{down()} and \texttt{up()} are \bold{atomic} (i.e. once the operation has started, no other process can access the semaphore until completed or blocked). \\ 

    \noindent The value of the semaphore... if \bold{positive}, represents the number of resources available and that there are no pending wakeups. If \bold{zero} represents that there are no resources available and no pending wakeups. If \bold{negative} represents that there are no resources available and signifes the number of pending wakeups (number of processes waiting). 

    \subsubsection{Types of Semaphores}

    \bold{Counting semaphore} is a sempahore based on the number of controlled resources. The value is an integer. \\ 

    \noindent \bold{Binary Semaphore} is a sempahore with a value of 0 or 1 which implementes mutual exclusion; this is called a \bold{mutex}. The value is either a 0 or 1. A value of 0 represents \bold{locked} and that the cirtical region is not available. A value of 1 represents \bold{unlocked} and that the critical region is available.

    \subsubsection{Producer-Consumer Problem using Semaphores}

    \begin{lstlisting}[language=C]
    Shared variables
    const int N=100;
    semaphore empty=N, full=0;

    Producer
    while(1) {
        produce an item A; 
        down(emtpy);
        insert item;
        up(full);
    }

    Consumer 
    while(1) {
        down(full);
        remove item;
        up(emtpy);
        consume an item;
    }
    \end{lstlisting}

    \noindent The sum of the semaphores must be equal to $N$. There could be a race condition with the insert and remove actions: if the buffer is not empty or full, then they may both try to insert/remove at the same time. \\
    
    \subsubsection{Producer-Consumer Problem using Semaphores and Mutexes}

    \begin{lstlisting}[language=C]
    Shared variables
    const int N=100;
    semaphore empty=N, full=0;
    mutex mux=1;

    Producer
    while(1) {
        produce an item A; 
        down(emtpy);
        lock(mux); // down(mux)
        insert item;
        unlock(mux); // up(mux)
        up(full);
    }

    Consumer 
    while(1) {
        down(full);
        lock(mux); // down(mux)
        remove item;
        unlock(mux); // up(mux)
        up(emtpy);
        consume an item;
    }
    \end{lstlisting}

    The order of the \texttt{up()} and \texttt{down()} calls is important and can't be changed. 

    \begin{center}
        \bold{Producer} = \texttt{down(empty), down(mux), up(mux), up(full)} \\ 
        \bold{Consumer} = \texttt{down(full), down(mux), up(mux), up(empty)}
    \end{center}

    \subsubsection{Monitors}

    Monitors are a higher-level primitive. Monitors are a programming language construct: a package or module or class. Some \bold{rules for monitors} include:

    \newlist
    \b only one process/thread can be active in a monitor at any time
    \b processes cannot access internal data of monitor (fields are private)
    \b put all critical sections inside monitor methods
    \endlist

    \noindent A \bold{mutex} is used to achieve mutual exclusion: locked/unlocked automnatically at function/method invocation and return. They also have a \bold{condition variable} which is used for processes to wait/block when attempting to enter a monitor which is already currently in use and to wake up the process. \\

    \noindent \texttt{wait(condition)}

    \newlist
    \b adds process to wait queue for condition variable
    \b causes the calling process to block
    \b releases the mutex to allow other processes to enter monitor 
    \b re-acquire mutex once blocked process is woken up
    \endlist 

    \noindent \texttt{signal(control)}
    \newlist
    \b wakes up one blocked process on the condition, if any
    \b exits the monitor, leaving critical section
    \endlist 

\subsection{Readers-Writers Problem}

    Problem is that multiple processes / threads need to acces a shared resources concurrently (but some only need to read it). Mutual exclusion allows one process to access the resource at a time, but can we allow more than one reader at a time? We want multiple readers to access the resource concurrently, but writers must access it with mutual exclusion. 

    \begin{lstlisting}[language=C]
    typedef int semaphore;
    semaphore mutex = 1;
    semaphore db = 1;
    int rc = 0;

    void reader(void) {
        while(TRUE) {
            down(&mutex);
            rc = rc + 1;
            if (rc == 1) down(&db)
            up(&mutex)
            read\_data\_base();
            down(&mutex);
            rc = rc - 1;
            if (rc == 0) up(&db);
            up(&mutex);
            use\_data\_read();
        }
    }

    void writer(void) {
        while(TRUE) {
            think\_up\_data();
            down(&db);
            write\_date\_base();
            up(&db);
        }
    }
    \end{lstlisting}

\newpage
\section{Memory Management}

Physical memory (RAM) is a hardware arary consisting of words. \bold{words} are a fixed-size unit of data (64 bits). When there is \bold{one program} with \bold{no memory abstraction}, the program lives in and directly works with physical memory; the program can even share physical memory with the OS. When there are \bold{multiple programs} with \bold{no memory abstraction} every program operates in physical memory, there is no isolation and thus programs can access each other's memory and OS memory. \\ 

Problems with having multple programs and no memory abstraction include:

\newlist 
\b No protection
\b expensive relocation process
\b security
\b \bold{stability}: user program could write over something in OS
\endlist

Additionally, note that programs must be \bold{relocatable}: written to be placed and run at any physical memory address. The \bold{loader} is the part of the OS that loads the program from disk to RAM and is reponsible for deciding where to place programs based on the available physical memory.  \\ 

\subsection{Memory Management}

The \bold{virtual address space} abstracts physical memory space so that each process has its own memory address space. \\ 

\subsubsection{Base and Limit Registers}

One such tool for implementing virtual memory is the use of \bold{base and limit registers}. These are hardware support to ease relocation (store the base register and limit register). Thus, the program does not need to be relocatable, so there is a faster load time. Each process also gets its own private address space. In this implementation, the memory management unit will convert virtual address in program (offset) to the physical memory address, then retrieve the data from the memory and return it to the user program. 

\subsubsection{Swapping}

\begin{enumerate}
    \item Save address space of idle processes to disk and reclaim memory
    \item When a swapped process needs to run, bring its address space from disk back to memory
    \item Go back to step 1
\end{enumerate}

Know that idle processes are mostly stored on the disk, so they do not take up any memory when they are not running. Swapped processes are kicked out of memory and brought back into memory in full when it is needed again. 

\subsubsection{Memory Fragmentation}

\bold{Memory fragmentation} happens during process creation, exit, and swapping when memory holes are created. This brings about the problem that new processes may not fit in any available memory hole, even if all the holes together are big enough for the new process. A solution to this is \bold{compaction}. In \bold{compaction} all the processes are moved tightly together, which results in one single hole which is big enough for the new process. Compaction has to freeze all the programs to perform the operatino, so it is not good to use on interactive systems where the freeze will be noticeable. \\ 

\subsubsection{Determine which Memory is Allocated}

With all implementations for memory managment, recall that we need to allow extra room for process growth. To figure out which part of memory is allocated, divide the memory into \bold{blocks}. $$\textrm{1 block} = 4 \textrm{KB}$$

\paragraph{Bitmaps}

Allocation units (blocks) are as small as a few words and as large as many KB. Each allocation unit has a bit in the bitmap which is 1 if that block is occupied and 0 if that block is free. An issue with this implementation is the long search time. 

\paragraph{Linked Lists}

Each node in the list contains a process or a hole. Each node has a start address and length. Since the list is sorted by increasing addresses, it's easy to merge adjacent nodes when a process exits. 

\subsubsection{Picking Location of New Process's Memory}

When picking where to allocate the memory of a new process, there are many options. The following are some of the common options:

\newlist
\b \bold{First fit}: take the first fitting hole. This is the simplest option. Tends to allocate at the beginning of memory
\b \bold{Next fit}: take the next fitting hold. Faster than first fit in practice (allocates from all over the memrory) but is not as memory efficient.
\b \bold{Best fit}: take the best fitting hole. This is slower as it must search the whole list. Prone to creating lots of small holes. 
\b \bold{Worst fit}: take the worst fitting hole. Poor performance in practice
\b \bold{Quick fit}: keep hole lists for holes of different sizes. Poor coalescing performance.
\endlist

\subsubsection{Overlays}

Overlays are a strategy where you only load part of the program. The programmer breaks the address space into pieces so that each piece can fit into physical memory. The pieces are called \bold{overlays} and are loaded and overwritten by the program when necesssary. This requires an overlay manager which likely takes the form of some software library. In this library, programmers call functions of the overlay manager to load an overlay segment into memory when it is not in RAM. This overwrites a segment previously in RAM. The \bold{issues} of this impelmentation are that it is implemented in the program itself and therefore there is a high complexity for the developer. However, this is still used in some embedded systems where the hardware does not support paging. 

\section{Virtual Memory}

\bold{Virtual memory} is a powerful concept for the address-space abstraction that decouples process address space from physical memory. It gives the illusion of large and private addresss spaceses. It also allows for programs to execute even if only partially loaded in memory. It enables different memory protection policies as well. In this, each process has its own virtual address space from 0x00 to some maximum. The program and CPU operate on virtual addresses which are translated to the physical addresses: a process which is transparent to the program. \\ 

The virtual address space is broken into \bold{chunks}. Each chunk is a contiguous range of addresses mapped onto a contiguous range of physical memory. The process always sees the entire virtual address space. Not all chunks need be in physical memory to run the program. Chunk switching is hidden from the process. When a program references a chunk loaded in physical memory, hardware peforms necessary virtual-to-physical address translation on the fly. When a program references a chunk not in physical memory, the kernel gets it from the disk and re-executes the memory instruction a second time. A chunk is either a \bold{segment} or a \bold{page}. \\ 

\subsection{Paging}

A \bold{virtual address space} consists of fixed-size chunks called \bold{pages} which each correspond to chunks in the \bold{physical memory} called \bold{frames}. Pages and frames are \emph{always} the same size (which is usually 4KB). The hardware \bold{memory management unit (MMU)} translates pages to frames; this translation is invisible to the process and the programmer does not have to worry about it. \\ 

The \bold{application} sees a single, private, contiguous addresss space while in reality application code and data are scattered across physical memory. Note that not all pages neeed to be loaded into physical memory. 

\subsubsection{Translating Addresses when Paging}

To translate the address when paging, the virtual address is broken into \bold{page number} and \bold{page offset}. \bold{Page number} is used to get the \bold{frame number}. This uses a mapping function and is implemented with a page table. The page number is used as the index into the page table. Each page table entry stores a frame number plus some additional information. The numner of entries in the page table is equal to the size of the virtual address space divided by the page size. 

\subsubsection{Page Table Entry}

The \emph{exact} content of a \bold{page table entry} is highly OS and MMU depenedent; however, all must contain at least the frame number. The entry will also have a \bold{modified/dirty} bit (referenced bit) which is set by the MMU when the page is used to allow the kernel to keep track of page usage. The entry also has \bold{protection bits} (i.e. what kind of access is permitted: read, write, execute). The entry also has a \bold{present/absent bit}: 1 if the entry is valid and page can be used, 0 if the page is not currently in memory. Accessing a page with absent/invalid entry causes a \bold{page fault}. 

\subsubsection{Page Faults}

If a process makes a reference to a page which is not currently in RAM, the reference to that page will result in the MMU detecting that the page is marked as invalid in the process' page table, and the MMU will trap to the kernel: \bold{page fault}. 

\begin{enumerate}
    \item Kernel looks at another table in the process' PCB to decide
        \newlist 
        \b If invalid reference, then abort
        \b If just not in memory, but exists somewhere else
        \endlist
    \item Kernel finds a free frame in RAM
    \item Page-in the needed page into the free frame in RAM using a disk operation
    \item Kernel changes the page table to indicate that the page is now in memory: set the valid bit to true
    \item Kernel restarts the process at the same CPU instruction that caused the page fault
    \item The second time the instruction will succeed without a page fualt. 
\end{enumerate}

During a process context switch, the kernel tells the MMU where in RAM to find the page table for the process executing next on the CPU. Everytime the process reads/writes a virtual addresss...

\newlist 
\b MMU uses the page number bits of the virtual address to lookup in the page table the corresponding frame number
\b If a page table entry is found, the MMU uses that frame number and the offset bits to generate the physical memory request 
\b If a page table entry is not found then a page fault is generated. 
\newlist 
\b Either the kernel kills the process if it was trying to access nonexistant memory
\b Or the kernel loads the requested page from disk and updates the page table before resuming the prtocess's execution
\endlist
\endlist

\subsubsection{Addresses}

In an \emph{n-bit} virtual addresss space, a virtual address consists of \emph{n} bits and there are $2^n$ virtual addresses. \\ 

In an \emph{n-bit} virtual address space with \emph{k bits} for the page offset. Each page consists of $2^k$ virtual addresses. The virtual page number takes $(n-k)$ bits. There are $2^{(n-k)}$ virtual pages and $2^{(n-k)}$ entries in the table. \\ 

In an \emph{m-bit} physical address space, a physical address consists of \emph{m bits}. There are $2^m$ physical addresses. \\ 

In an \emph{m-bit} physical address space with \emph{k bits} for the frame offset. Each page consists of $2^k$ physical addresses. The frame number takes $(m-k)$ bits. There are at most $2^{(m-k)}$ physical frames. \\ 

\subsubsection{Translation Lookaside Buffer}

The use of a \bold{translation lookaside buffer} helps reudce accesses to the page table by caching recently used page table entries. TLB is a hardware cache for page table entries inside the MMU, used to speed up address translation. This works because most programs often make a large number of references to a small number of pages: locality of memory references. \\ 

The \bold{translation lookaside buffer} translates virtual addresses into physical addresses without going through the page table. It is usually implemented as an \bold{associative memory} that caches most recent virtual to physical address translations. All TLB entries are searched in parallel. 

\newlist 
\b If the page number if found in the TLB: \bold{hit}. The matching TLB entry contains the right frame number and thus the virtual address can easily be translated to the physical memory. 
\b If the page number is not found in the TLB: \bold{miss}. The MMU access the page table in the main memory to get the right frame number. MMU translates the virtual address into a physical address. The MMU updates the TLB so accessing the same page later will then result in a hit. 
\endlist

\paragraph{Metrics}

\bold{Hit ratio} (\emph{h}) is the percentage of time that a page number is found in the TLB. \\ 

\bold{Effective access time} (EAT) = $a + (2-h)m$. Where $a$ is TLB lookup time, $m$ is memory access time. $a$ is negligible; if $h$ is close to 1 (100\%) then EAT is close to $m$. 

\subsubsection{Multilevel Page Tables}

There is a high memory overhead with a \bold{flat page table}. If the virtual address space is large then the page table will be large as well. Even if the program uses a small fraction of its virtual address space, the entire page table is still needed. \\

In \bold{multilevel page tables}, the virtual address bits are split into three parts: high page bits, low page bits, and offset bits. The basic idea is to \emph{page} the page table. This allows for only portions of a page table to be kept in memory at one time (the rest can be on the disk); portions of a page table can simply not be present at all. When there is a TLB miss, a process might now require three memory accesses: 

\newlist
\b primary page table needed by the MMU
\b Secondary page table needed by the MMU
\b Data requested by the process
\endlist

In practice, the TLB hit ratio ($h$) is very close to 100\% so requiring 3 memory accesses only happens very rarely. 
\paragraph{Example}

Assume the 32-bit address is allocated as following: 10 bits to the primary page, 10 bits to the secondary page, 12bits to the page offfset. \\ 

1st-level page size = $1024 \times 4 \textrm{ bytes} = 4\textrm{KB}$ \\ 

2nd-level page size = $2^{10} = 1024 \times 4 \textrm{ bytes} = 4\textrm{KB}$. The secondary page gets 10 bits, so 1024 unique addresses and each one is 4 bytes.

Total size of the page table = $4\textrm{KB} \times 1024 + 4\textrm{KB}$. All 1024 entries in the pruimary table point to one secondary page. 

Thus, 8KB of memory is needed for one virtual address translation. 

\subsubsection{Inverted Page Tables}

Another approach, instead of paging, is using \bold{inverted paged tables}. To do this, you must have a frame table where the frame number is the index and the page number is the context. The table size is proportional to RAM size, which is much smaller than the size of the virtual address space of a process. Only a single frame is required for all processes. The problem with this implementation is tha t a TLB miss requires searching the frame table entry by entry, so is very slow on a miss. This takes \bold{less space} than paging but \bold{more time}. \\ 

To implement an inverted page table using hashaed maps, hash the page number to a bucket. The table size only depends on the has function used. Hashing is slow but only needs to be done on a TLB miss. Multiple page numbers might hash to the same bucket, so then need to search a (short) list to find the right frame number. 

\newpage
\section{Page Replacement Algorithm}

If there is no free frame in physical memory, then must identify some \emph{victim} pages in memory and move them to disk to free the corresponding frames. \\

\bold{Question}: Which page to evict when physical memory is full and a free frame is required? \\
\bold{Goal}: Achieve the lowest number of (future) page faults \\ 
\bold{Input of Algorithm}: A particular string of memory references \\ 
\bold{Output of Algorithm}: The number of page faults on that string \\ 

\subsection{Optimal Algorithm}

The \bold{optimal algorithm} will replace the page in memory that will be used the furthest in the future. This is the page that will create a page fault the furthest in the future, thereby minimizing the number of page faults. This is not achievable in real systems because this only works if we know the whole sequence of page references in advance. \\ 

In the following algorithms, we will use the following notation. In \bold{page table entries}, \bold{R} is set whenever the page is referenced, \bold{M} is set when the page is written to. 

\subsection{Not Recently Used Algorithm}

The general idea of this algorithm is to use virtual memory hardware tracking bits to determine what page was not recently accessed. 

\begin{enumerate}
    \item When a process starts, the R and M bits for all its pages are set to 0 by the kernel
    \item Periodically (on the clock interrupt), the R bit is cleared (the M bit remains the same). 
        \newlist 
        \b To distinguish pages that have not been referenced recently from those that have been 
        \endlist
    \item When a page fault occurs, the kernel inspects all the pages and divides them into four categories
        \newlist 
        \b \bold{Class 0}: not referenced but not modified
        \b \bold{Class 1}: not referenced but modified
        \b \bold{Class 2}: referenced but not modified
        \b \bold{Class 3}: referenced and modified
        \endlist
    \item The algorithm removes a page at \bold{random} from the lowest-numbered non-empty class
\end{enumerate}

\bold{Disadvantages} of NRU algorithm:
\newlist 
\b only looks at referenced/modified bits chagned since the last clock tick
\b pages split among only four categories
\b performance is, at best, adequate
\endlist

Note that the page removed is from the same process that triggered the page fault in the first place. 

\subsection{First in First Out (FIFO) Algorithm}

In FIFO, the kernel maintains a linked list of all pages in the order that they came into physical memory. The page at the beginning of th elist is removed from the list and from physical memroy when a free frame is needed; this is also the page that has been in physical memory for the longest time, so may very well be a frequently used page. \\

An \bold{advantage} of FIFO algorithm is that it is easy to implement. \\ 

\bold{Disadvantages} of FIFO algorithm:
\newlist
\b The page which is memory the longest is replaced. This does not consider page usage and thus may throw out pages that are in use right now. 
\b Suffers from "\bold{Belady's Anomaly}": the page fault rate might increase when there are more frames
\endlist

The linked list can contain pages from many different processes, so the page removed might not belong to the process that triggered the page fault. 

\subsection{Second Chance Algorithm}

Second chance is a FIFO variant that adds the concept of page usage (check reference bit). This examines pages in FIFO order starting from the beginning of the linked list but considers the reference bit, R of the page at the front of the list:

\newlist 
\b If \bold{R=0}, the oldest page has not been used since it was added at the end of the list, so remove page (go to third bullet)
\b If \bold{R=1}, set R=0 and place the oldest page back at the end of FIFO list (second chance) because it was used recently (go to first bullet)
\b Add new page at the end of FIFO
\endlist

If no victim page is found on the first pass, the algorithm automatically reverts to pure FIFO on the second pass. The problem with the second chance algorithm is that moving pages around the list is not efficient. The \bold{clock algorithm} is a more performant implementation that uses a circular list and simply moves a pointer around the list. 

\subsection{Least Recently Used (LRU) Algorithm}

LRU is a good approximation of the optimal algorithm that is based on the obsevations that pages that have been heavily used in the last few instructions will probably be heavily used again soon. Additionally, we can infer that pages that have not been used for a long time will probably remain unused for a long time. When a page fault occurs, remove the page that has been unused for the longest time in the past. 

\subsubsection{Priority List Implementation of LRU}

Keep a doubly linked list of page numbers with the most recently used at the front and the least recently at the rear. A page that gets referenced is moved to the front, and on a page fault, you always remove from memory the page at the rear of the linked list. In this implementation, it is easy to find the least recently used page, but a list update is required on every memory reference; this is time consuming and not practical. 

\subsubsection{Counter Implementation of LRU}

Equip the hardware with an N-bit counter and on every instruction that references memory, increment the counter. Every page table entry must have a field to save the counter. When a page is referenced, copy the counter to that field. The more recently that the page is used, the larger the counter value is in its table entry. For replacement during a page fault: evict the page with the lowest counter value (i.e. the oldest page). This operation requires searching the page table for the entry with the lowest counter, so it is linear time in the size of the page table. This also means that the evicted page always belongs to the process that triggered the page fault.

\subsection{Not Frequently Used (NFU) Algorithm}

\begin{enumerate}
    \item At each clock interrupt, for each page, the R bit is added to a software counter for each page which keeps track (roughly) of how often each page has been referenced in the past 
    \item The page with the lowest counter is evicted during page fault
\end{enumerate}

This algorithm never forgets: looks at overall usage of each page rather than recently used usage. 

\subsection{Aging Algorithm}

This algorithm is a variant of NFU that forgets the past after a while. Uses an N-bit counter per page and periodically...

\newlist 
\b Shift couter to the right to forget the oldest bit: reduces counter values over time by dividing it by two
\b Add R as the new leftmost bit to represent recent usage. This recent R bit is added as the most significant bit and, therefore, more weight is given to recent references
\endlist 

When a page fault occurs, we replace the page with the lowest counter. Because this algorithm uses 8 bit integers, it only remembers what has happened within the last 8 clock ticks (anything older than that would be shifted to zero). If two pages have a value of 0 then we know they have not been used for $n$ clock ticks, but we do not know which of the two was used more recently. 

\subsection{When to Move Pages into Memory}

There are many options of how to decide when to move pages into memory. Here are some of the options. \\ 

\bold{Demand} paging is when pages are loaded on demand, not in advance \\ 

\bold{Prepaging} loads a group of pages at once in the hope of minimizing the numbner of page faults later. The group of pages to get is a guess but could include all of a given program's pages, the first/last $N$ pages: the \bold{working set pages} \\ 

\subsubsection{Working Set Pages}

The \bold{working set} is the set of pages that a process is currrently using. This set may change over time in the regard that there will be different pages and that there will be a different total size due to the pages themselves changing. The working set of pages are those used by the $k$ most recent memory references. $w(k,t)$ is the size of the working set at time $t$. \\ 

If the entire working set is in physical memory, there are no page faults. If there is insufficient physical memory for the working set, \bold{thrashing} where a process busies itself copying pages in and out of memory. The goal is to keep the working set in physical memory to minimize the number of page faults. 

\subsection{Working Set Based Algorithm}

This page replacement algorithm has a threshold T (working set window size), and for every page table entry keeps track of the time of last use, reference bit R and modified bit M. During a page fault, the algorithm will scan the page table, and for each entry: 

\newlist 
\b If \bold{R=1}, set time of last use to current time, set R to 0. Page has been referenced since last clock tick, so in working set, so remember when it was last used. 
\b If \bold{R=0}, the page is a candidate for removal. First calculate the age: $\textrm{age} = (\textrm{current time} - \textrm{time of last use})$. If the age is greater than threshold T, then the page is replaced. If the age is less than threshold T, the page is still in the working set, but may be removed if it is the oldest page in the working set. 
\b If not page has R=0, choose the oldest and when they are all the same age, pick randomly
\endlist

This algorithm works fine in terms of page faults, but requires scanning the whole page table which is expensive. 

\subsection{Working Set Clock (WSClock) Algorithm}

This algorithm uses a circular list of \bold{frames} (including R and M bits and time of last use). When there is a page fault: 

\newlist
\b If \bold{R=1}, set R=0 and advance the clock hand around the list. This page was used recetnly and is part of the working set. 
\b If \bold{R=0} and \bold{age is greater than threshold T}:
\newlist 
\b Page is clean (M=0), replace this page
\b Page is dirty (M=1), schedule write but advance hand to check other pages
\endlist
\b At the end of the first pass, if no page has been replaced...
\newlist
\b If the write has been scheduled, keep moving the hand until the write is done and the page is clean. Evict the first clean page. 
\b If no write scheduled, claim any clean page even though it is in the working set. 
\endlist
\endlist

This algorithm is simple and has good performance so is used a lot in practice. 

\subsection{Frame Allocation to Processes}

Each process requires a minimum numnber of frames. The following are some of the options for choosing how many frames to allocate.

\subsubsection{Fixed Frame Allocation}

In this implementation there could be \bold{equal allocation}; so, if-for example-there are 100 frames and 5 processes, then you give each process 20 frames. There could also be \bold{proportional allocation} which means that we allocate frames according to the size of the process. 

$$S \sum_{}^{} s_i$$
$$S_i = \textrm{ virtual size used by process } p_i$$
$$M = \textrm{ total number of frames}$$ 
$$a_i = \textrm{ allocation for } p_i = \frac{s_i}{S \times m}$$

\subsubsection{Priority Allocation}

This implementatnion uses a proportional allocation scheme that uses priorities rather than size. 

\subsubsection{Local Replacement}

In this implementation, for each process, the kernel only selects from the process's own set of allocated frames; therefore a process has a fixed number of frames. The good thing about this is that when a process thrashes, it does not cause other processes to thrash. The bad thing about this is that it can lead to wasted memory if a process's working set decreases in size. 

\subsubsection{Global Replacement}

For each process, the kernel selects a replacement frame from the set of all frames. This is better for dynamic working set which may grow or shrink. However, there can be domino-style thrashing. Most modern operating systems use global replacement to avoid wasting memory. 

\paragraph{Avoiding Thrashing}

To prevent thashing, adopt a local allocation policy (wastes memroy), do not schedule a process unless its working set of pages is in memory (implies prepaging), and use a page fault frequency approach. 

\paragraph{Page Fault Frequency Scheme}

Measure the page fault rate of process and if its too low then take pages from that proces and if too high, then allocate additional frames to the process. If many processes have high page fault frequency, swap out one or more of these processes (the process to swap out is determined by the scheduling algorithm). This may use process priority to affect the range of acceptable page fault frequencies. 

\newpage 
\section{File Systems}

File systems are used for long term information storage, i.e.

\newlist 
\b It must be possible to store a very large amount of information
\b Information must survive termination of a process using it
\b Multiple processes must be able to access information concurrently
\endlist 

A \bold{file} is ab abstraction that is a logical unit of information created by processes managed by the operating system. The \bold{file system} is the subssytem that deals with files (part of the kernel). In UNIX, extensions are just conventions that sometimes determine how a file should be opened, but are mostly information for the user: the kernl does not care. \\ 

To read a byte using \bold{sequential access}, all previous bytes must be read first. To read a byte using \bold{random access}, you can read any byte at any time. \\ 

\bold{Directories} are persistent data structures for organizing and maintaining information about files (they are also files themselves). The \bold{file system} consists of the kernel's file system subsystem organized into layers and teh file system data is stored in storage. 

\subsection{Disks}

A \bold{disk} can be divided up into one or more partitions each with an independent file system. The \bold{master boot record (MBR)} is a 0.5 KB partition that looks at the partition table and decides which to use. The \bold{partition boot record} finds the boot loader for the kernel: there is a PBR in each partition. To boot...

\newlist 
\b The computer's BIOS runs first, finds boot storage device, and loads the MBR
\b MBR contains partition table and boot code that finds and loads PBR
\b Boot block = PBR = boot code to find kernel loader inside the file system of the partition (which itself then finds the kernel) 
\b Superblock stores key parameters about the file system
\endlist 

\subsection{Allocation Methods}

A \bold{disk block} is the minimal unit of allocation. Physical addresses are some kind of tuple that identifies a single block on a disk. The kernel drive translates physical addresses to block addresses. The logical file address is relative to the beginning of the file and is measured in bytes. 

\subsubsection{Contiguous Allocation}

In contiguous allocation, each file occupies a set of contiguous disk blocks. Entire blocks are used independent from the file size (i.e. file size is not a consideration). \\ 

\bold{Pros} of contiguous allocation:

\newlist 
\b \bold{Simple}: only need to maintain starting location and length
\b \bold{Performance excellent}: the entire file system can be read in a single operation
\endlist 

\bold{Pros} of contiguous allocation:

\newlist 
\b \bold{Wasteful of space}: external fragmentation, may require compaction
\b \bold{Files cannot grow}: need to know the final size upfront
\endlist 

\subsubsection{Linked List Allocation}

In linked list allocation, each file is a linked list of disk blocks. THe first bytes of the block contain a pointer to the next block. Blocks may be scattered anywhere on the disk. \\

\bold{Pros} of linked list allocation:

\newlist 
\b \bold{Simple}: only need starting address
\b \bold{Free-space management system}: No waste of space
\endlist

\bold{Cons} of linked list allocation: \bold{No efficient random access}: must go from block to block to find the data

\subsubsection{Linked List Allocation with Table}

This implementation is a variant of linked list allocation that uses a \bold{file-allocation table (FAT)}. We keep the pointer in a separate table rather than as part of data blocks. In the table, each entry corresponds to the disk block number and contains a pointer to the next block. The table is on disk but copied into the memory for speed. \\ 

\bold{Pros}: Faster raandom access

\bold{Cons}: Entire table must be in memory (huge problem for very large disks)

\subsubsection{Indexed Allocation}

In indexed allocation, we associate a block-sized data structure called index-node (i-node) to each file which lists the attributes and lists the disk addresses of the file's blocks. The last address(es) is for another blcok of addresses. \\

\bold{Pros} of indexed allocation:

\newlist 
\b efficient random access
\b dynamic access withotu external fragmentation
\b lower overhead than FAT
\endlist

\bold{Con} of indexed allocation is that it can be hard to support large files (that do not fit the i-node) \\ 

An i-node can contain a pointer to direct block (data blocks), single indirect, double indirect, triple indirect block, etc... \\ 

\bold{Indirection} allows files to grow and be incredibly large. Indirection refers to when i-node points to indirection blocks which can point to other indirection blocks or data blocks. 

\subsection{Implementation of Directories}

To implement directories we first start by mapping the file name to some attributes and data. Directories are stored as special files: the file contents are the directory entries. \bold{Directory blocks} include on entry for ecah file within that directory; directories with many files may span multiple blocks. Every file and directory is represented by an \bold{i-node} (index node). This index node contains two kinds of information:

\newlist 
\b Metadata describing the file or directory's owner, access rights, etc...
\b Location of the file or directory's data blocks on disk
\endlist 

For a directory, the data blocks simply store the directory's file entries: file name + corresponding i-node number. The beginning part of the file system contains all the inodes. \\ 

Because it can be convenient for a shared file to appear in different directories, the directory system is implemented as a \bold{directed acyclic graph}. We can thus create multiple names for the same inode in the same directory or even in different directories. All the names must be on the file system since each file system has its own inode. In UNIX and Windows NTFS this is called a \bold{"hard link"}, which refers to the specific location of physical data. Use the Linux commnad \code{ln <source> <destination>} to create a link. \\ 

\subsubsection{Symbolic Link}

A \bold{symbolic link} is a special kind of file, the contents of which indicate the location of another file. These files have a special status that is indicated by the file attribute "sym link" bit being set. During name interpretation, symbolic link is read nd used to find the read file. You can use the Linux commmand \code{ln -s <source> <destination>} to create a symbolic link. To read the contents of a symbolic link file itself, you can use the \code{readlink} command. Only the original file actually links to the i-node; symbolic links only link to path names, not i-nodes. When the owner removes the real file, the link now points to a non-existent path name. \\ 

\bold{Disadvantage}: extra overhead to parse symbolic links

\bold{Advantage}: can link across file systems. 

\subsection{Virtual File System}

Many different file systems are in use even onthe same computer and under the same operating system. From the user's point of view, there is a single file-system hierarchy which encompasses multiple (incompatible) file systems. The UNIX kernel uses a \bold{virtual system layer} (VFS) which integrates multiple file systems into a single structure and abstracts over the details of each file system. All system calls related to file are directed to the VFS layer using standard POSIX calls. The lower VFS interface calls into the file system code. Internally, the VFS is object oriented and contains a superblock, v-node, and directory. 

\newpage
\section{Input/Output}

The operating system...

\newlist 
\b \bold{Controls} all the computer's I/O devices
\newlist 
\b Issues commands to the devices
\b Catches and manages interrupts
\b Handles errors
\endlist 
\b \bold{Provides an interface} between the devices and the user-space software that is simple, easy to use, and potentiall the same for all devices
\endlist 

One type of device that may perform I/O with the system are \bold{block devices} which store informatin in fixed-size blocks. In block devics, each block has its won address and transfers are in terms of entire blocks. 

Another type is a \bold{character device} which delivers or accepts a stream of characters without any block structure. These are not strictly addressable and do not have any \emph{seek} operations. 

\subsection{I/O Hardware}

A \bold{port} is a connection point for a device. There a \bold{buses} such as {peripheral buses} such as PCI and PCIe and \bold{expansion buses} which connect relatively slow devices. There will be a \bold{controller (host adapter)} which operates the port, bus and devices. Sometimes this controller will be integrated on the motherboard while others times it may be on a separate circuit board. The controller contains the processor, microcode, private memory, bus controller, etc... \bold{Buses} are used to handle traffic between I/O devices and the processor. \bold{PCI/PCIe} connects with speed graphics, networking, etc and connects to low speed buses. \\ 

Each device has both a mechanical and electronic component. The \bold{mechanical component} (the device itself) will have its own electronic controller too, but this is invisible to the kernel. The \bold{electronic component} (chip on the motherboard) consists of the controller (host adapter) which may be able to handle multiple or only a single device at once. The controller's tasks are to convert bit streams to bytes, perform error correction when necessary, and move data to/from host memory. 

\subsection{Communication Mechanisms Between CPU and I/O Devices}

Communication between the CPU and I/O devices can happen through registers or a buffer. The kernel device driver reads/writes to \bold{registers} for configuration/initialization, reading/writing data, and to check the status of a device. The kernel device drive reads/writes to \bold{buffers} to move data. 

\subsubsection{Register Based I/O}

Each controller register has an I/O port number. There are special instruction to access the I/O port space: CPU reads in from device I/O port to CPU registers (\code{IN REG, PORT}) and the CPU writes to device I/O port from CPU register (\code{OUT PORT, REG}). These instructions are priveleged (kernel-use only) and must be written in assembly. There are separate I/O port space and memory space. 

\subsubsection{Memory Mapped I/O}

All controller registers and buffers are mapped into the normal RAM address space. Each controller register is assigned a unique memory address: there is not actual RAM memory for that address. Such addresses may be at the top of the physical address space. \\ 

\bold{Advantages} of memory mapped I/O:

\newlist 
\b Software can read/write to I/O deivces as to any other memory location. All normal load/store memory instructions apply to I/O devices as well. Read/write easy to program in C using a normal pointer. 
\b Protection of I/O device registers and buffers is the same as memory protection. Can be accessed from kernel mode and user mode (if you have the right permissions). 
\endlist 

\bold{Advantages} of memory mapped I/O:

\newlist
\b Need to disable cachine to memory-mapped I/O
\b For one address, RAM controller and I/O devices will examine the memory reference. 
\endlist

\subsubsection{Hybrid I/O Implementation}

A \bold{hybrid I/O implementation} uses both I/O ports and memory-mapped I/O. To \bold{send data to I/O device}, the CPU wants to read a word either from memory or I/O port:

\begin{enumerate}
    \item Puts the address it needs on the bus address line
    \item Asserts a read signla on a bus control line
    \item A second signal line to tell I/O space or memory space
\end{enumerate}

\newlist 
\b If it is memory space, memory responds to request
\b If it is I/O space, I/O device repsonds to the request
\b If there is only memory space (no I/O ports)
\newlist 
\b RAM controller and every I/O device compares the address lines to the range of addresses that it services
\b If the addresss falls in its range, it responds to the request
\b Since no address is ever assigned to both memory and I/O device, there is no ambiguity and no conflict
\endlist 
\endlist 

The CPU can request data from an I/O controller on byte/word at a time; this wastes the CPU's time for big transfers. In \bold{offloaded communication}, the CPU can offload data transfers and do something else in the meantime. 

\subsubsection{Direct Memory Access (DMA)}

The \bold{direct memory access} (DMA) controller transfers data for the CPU from/to the I/O devices and between I/O devices. This implementation requires a DMA controller on the device host controller or on the motherboard. A \bold{DMA controller} contains registers to be read/written by the kernel include the memory address register, bute counter register, and control registers to indicate the direction of the transfer and the transfer unit. The CPU commands the DMA controller by writing to control registers of DMA device. DMA is no faster than CPU but frees the CPU from having to wait for everything to finish and thus is used to take care of all big memory transfers. To do \bold{multiple data transfers} at once (multiple channels), we need multiple sets of registers (one for each transfer channel). 

\paragraph{Data Transfer Bus Mode}

In \bold{word-at-a-time} mode, the DMA controller requests for tansfer of one word. If the CPU wants the bus, it has to wait: this is called \bold{cycle stealing}. \\ 

In \bold{block} mode, the DMA controller acquires the bus, issues a series of transfers, and releases the bus: this is also called \bold{burst} mode. An advantage to block mode is that it takes less time for acquiring the bus. A disadvantage of block mode is that it can block the CPU and other devices for a long time if there is a long burst. \\ 

In \bold{fly-by} mode, the DMA controller demands the device controller to transfer data directly to the destination. \\

In \bold{flow-through} mode, data transferred passes through the DMA controller. The DMA controller first reads data into an ainternal buffer and then writes it to the destination. This scheme requires an extra bus cycle per word transferred, but is more flexible because it can perform both device-to-device and memory-to-memory copies. \\


\paragraph{Interrupts}

DMA controllers are able to read and write \bold{directly from/to memory} and/or I/O devices with not kernel intervention in the meantime. DMA hardware enables data transfer to be accomplished without using the CPU except for configuration. Interrupts are sent to the interrupt controller from the device; the controller issues the interrupt to the CPU and the CPU acknowledges the interruipt. \\ 

There are type types of interrupts: external interrupt and internal interrupts (referring to whether the interrupt comes from within our outside of the CPU). \bold{Processing} any type of \bold{interrupt} is almost identical: 

\begin{enumerate}
    \item Kernel (with help from CPU) is responsible to save the state of the interrupted process
    \item Kernel processes the interrupt
    \item The interrupted process is resumed after the interrupt is handled by the kernel. 
\end{enumerate}

\bold{External interrupts} deal with interactions with the I/O devices. \\

\bold{Internal interrupts} (traps, exceptions) occur entirely within the CPU. They are used to handle exceptional conditions that occur during the execution of programs such as divide by zero. \\ 

\paragraph{Processing Interrupts}

The CPU checks for interrupts after executing every instruction. In the case that there is an interrupt, the hardware looks up the \bold{interrupt vectory} (configured by the kernel at boot time) to fetch a new program counter. The new program counters points at the code of the right kernel \bold{interrupt handler} which is then executed. 

\subsection{Goals of I/O software}

Some of the goals of I/O software include the following:

\newlist 
\b \bold{device independence}: programs can easily access any I/O device without having to specify the device in advance.
\b \bold{Uniform naming}: name of a file or device should be a string independently of device
\b \bold{Error handling}: errors should be handled as close to the hardware as possible
\b \bold{Synchronous} (blocking) vs. \bold{asynchronous} (interrupt-driven)
\b \bold{Buffering} involves considerable copying which may negatively impact I/O performance
\b \bold{Shareable} vs. \bold{Dedicated devies}: Some I/O devices, like disks, can be used by many users at the same time while other devices, such as printers, have to be dedicated to a single user.
\endlist 

\subsubsection{Programmed I/O}

In programmed I/O, the CPU does all the work. The CPU writes/reads a byte/word at a time from/to main memory to/from the device. The CPU makes a request, then waits for the device to become read. Buses are only byte/word wide, so the last few steps are repeated for large transfers. \\ 

CPU time is wasted; if the device is slow, the CPU may wait a long time. The CPU continuously polls the device to see if it is ready to accept another byte/word: \bold{polling} or \bold{busy waiting}. 

\begin{lstlisting}[language=C, caption=Writing a string to the printer using programmed I/O]
copy_from_user(buffer,p,count);             /* p is the kernel buffer */
for (i=0;i>count;i++) {                     /* loop on every character */
    while(*printer_status_reg != READY);    /* loop until ready */
    *print_data_register = p[i];            /* output one character */
}
return_to_user();
\end{lstlisting}

\bold{Pros}: simple to implement/understand

\bold{Cons}: CPU cannot be used for anything else until I/O ends, polling is inefficient

\subsubsection{Interrupt-driven I/O}

In interrupt-driven I/O, the CPU does the work but interrupts tell when to do that work. We use interrupts because I/O devices are slower than memory or CPU and there is uncertaintly of when the device will be ready. The OS needs to know when the I/O device has completed an operation and when the I/O operation has encountered an error. \bold{Instead of waiting}, the CPU continues with other computations. The device, then, interrupts the processor when operatin completes or there is an error. 

\begin{lstlisting}[language=C, caption=Writing a string to the printer using interrupt driven I/O]
/* Executes at the time the print system call is made */
copy_from_user(buffer,p,count);
enable_interrupts();
while(*printer_status_reg != READY);
*printer_data_register = p[0];
scheduler();

/* Interrupt service procedure for the printer */
if (count == 0) {
    unblock_user();
} else {
    *printer_data_register = p[i];
    count = count - 1;
    i = i + 1;
}
acknowledge_interrupt();
return_from_interrupt();
\end{lstlisting}

\bold{Pros}: Better than programmed I/O because CPU can execute another process in the meantime. Also avoid inefficient polling

\bold{Cons}: An interrupt occurs on every character. Interrupts take time, so it wastes CPU cycles. 

\subsubsection{I/O using DMA}

In I/O using DMA, the DMA controller does all the work, it uses interrupts for notification, but CPU needs to program the DMA controller. The DMA controller reads/writes directly from/to memory. It has access to the system bus independent of the CPU.

\begin{enumerate}
    \item The CPU sets up the operatin
    \item DMAC does the transfer
    \item When the transfer is complete, the DMAC notifies the CPU with a single interrupt
\end{enumerate}

\begin{lstlisting}[language=C, caption=Printing a string using DMA]
/* Syscall code. Executed when the print system call is made */
copy_from_user(buffer,p,count);
set_up_DMA_controller();
scheduler();

/* Interrupt handler code */
acknowledge_interrupt();
unblock_user();
return_from_interrupt();
\end{lstlisting}


\end{document}
